[Music] welcome back in the previous videos as part of this week's lectures we looked at uh the topic of machine learning at Large Scale we started off by looking at what did machine learning at Large Scale but done on a single machine look like and there are options available for doing that uh for basically solving the problem in a single machine but there are um all obviously problem statements for which even a single machine is not sufficient that's when you need a cluster what kind of algorithms are suitable for that then we went into the discussion on how does spark help to make make this process much smoother smart machine learning libraries specifically ml lib we discussed that uh we discuss some nuances around best practices around use usage of ml lip U so that's where we stopped with the previous video in this video we start to look at one of the more advanced concepts which is called as mlops but it's actually becoming mainstream now um the reason it's becoming mainstream now is almost every institution has some application of machine learning and production and therefore they are facing this problem and have to solve for it so what is the problem statement um when you have um models which are operating in production over a long period of time what ends up happening is firstly you end up changing those models because you keep refining them you keep changing the parameters Etc secondly the data itself changes so you have to keep retraining models on fresher and fresher data because the data is actually a reflection of your business and the business has changed or the climate has changed and third uh the algorithms themselves are becoming more sophisticated so as a result the the model changes but also the Feature Feature engineering pipelines change um and lastly and not and definitely not the least of the factors is that the people change right the people in the commercial establishment changes and U the moment that change happens uh newer set of data scientists newer Engineers come in they uh change models they make their own choices of how something needs to be solved for so long story short lot of change happens and when this kind of a change needs to be managed and if you start doing this change management in a Excel sheet or in a notebook or a file um I mean physical notebook not Jupiter notebook U then you start to lose track of what what what actually happened in production so the most famous example of this was uh uh was something that happened in a bank um there was a ml model that was used as part of credit decisioning um and as part of credit decisioning there is a you know one of the critical input factors as uh you know is a person eligible to get credit um so there's a risk assessment that happens u in many cases the risk assessment is actually being done using machine learning um and then the uh decision is taken whether to give that person a credit card or whatever uh credit product that they might be interested in um so as with all such banking products uh regulation plays a critical role and the regulatory requirement is that that um any single such uh customer facing decision that is made by a bank or a lending institution needs to be um traceable and explainable right so which means you need to be able to say why did the decision get made and secondly what was the consequence of what was the sequence of activities uh as a consequence of which that particular decision was made so the sequence of activities means what model what data what parameters when was it invoked what time did the answer come back when did the customer get informed about it etc etc so what happened in this bank was that um there were a lot of models and the models were changing uh quite frequently like we spoke about um so as a result at one point in time the model that was running in production which was responsible for this credit deising um was something that uh uh stopped being you know as if as as effective as earlier uh when it was first St into production but the people who wrote the models were no longer there in the company right um so as a result nobody could actually go and change the model or take a look at what was happening under behind the scenes and nobody was even sure right which version of the code was actually product promoted to production and therefore which version of the model was actually running in production so at some point um the regulator caught onto it there is no trace of right there is no specific code that actually produced that model that anybody was authoritatively able to prove and that resulted in a hefty penalty um so these things are very very you know taken seriously in many verticals or you knows like financial institutions or Insurance Etc so uh definitely something to keep in mind uh why did the term mlops come into play U I think it's more a play on the word uh rather the face devops devops is the more classical and um popular cousin right uh it it originated from the the time of um effectively software engineering aile practices U devops refers to the fact that uh when software gets produced basically when development happens um there is a there's a lot of operational overheads that need to be managed um in order for that development to see the light of the day meaning in know in in order for the development to actually become um you know deployed in production and create the impact that it was supposed to create and as a result um if nobody really understood how that was happening how code was being promoted to production then you get this situation where uh potentially uh unowned code becomes responsible for driving business and that's very risky so devops became this practice of a set of practice I around how code needs to be built how how does it need to be tested and deployed and um so that became a practice of its own lot of you know operational best practices have come in as a result tooling as well so mlops is something similar because at the end of the day the code that produces ml algorithms are effectively similar to the software code that goes into applications um but there are some key differences right um so let's take a look at that in the in this particular uh video first and foremost what is MLS let's formally Define this it's the set of operational processes which manage ml models their code and the data that they rely on so unlike software which is basically just purely code here there are two other unique elements that come into play for mlops one is the model itself and uh of course the code right and then the data um so how does this come about first and foremost the code that that denotes the model is basically the algorithm along with the feature engineering pipelines the data pipelines Etc all of that put together is the code that code has to run on data in the first place there is input data right that's associated with it that input data could be volumous or not that doesn't matter but the fact is that data has a characteristic that data has a schema the data has a certain distribution so one example here would be um if you use a machine algorithm for classifying and let's say you picked on uh uh you know couple of different approaches possible um so one of the algorithms could be a neural network classifier another one could be a naive Bas classifier a basing classifier uh the data on which the classifier is is trained on has to have certain prerequisits in order for those algorithms to make sense so specifically for naive Bas classifier um the data can has to be identically and rather has to be identically and independently distributed um which means that um the data cannot be having correlations between each other so uh you might start your um analysis by verifying that the the correlation between variables of the independent variables is not there U but then over a period of time as your business changes as the new set of data gets collected uh these correlations might start coming in in the data set so at that time because the data is changed right the code that you have written might still be you know syntactically correct but the model that you're running in production which is the model that you've trained on this corelated data uh May no longer make sense right so that's why it's important to track the data along with the code when you start to think about what version of the model is in production what was it drained on Etc so we spoke about the code we spoke about the data on which the the model runs basically the feature engineering pipelines the model prerequisits the assumptions behind the algorithm all of those are um you know constraints that you put on the data and of course the model itself right so what I mean by model itself is at the end of the day the code produces an artifact right the code is basically a a set of like I said feature enging pipelines resulting in model getting trained once the model is trained the model is manifested as a pickle file or a jar or one of these common ways in which models get packaged and then once it's you know packaged in a certain way that artifact is what actually you know gets shipped to production for execution that's what I mean by model here the reason I'm making this distinction again is um running code over and over again might or might not be practical um let's take an example of deep learning we'll take a look at that in the next next uh week's lectures the code might actually be a very very complex piece of code right or the code might be simple but the data volumes are so so high the number of parameters could be in the billions like is the case with you know GPT like algorithms and you can't just keep running the code again and again every time you want to do something right um every time you want to you know use a prediction from that particular model because the cost of running the code at at that large a scale is actually quite High you don't need to keep training it basically right you can use train models and and deploy them in multiple places and move on so that's therefore the models themselves have to be trct so set of operational processes to manage ml models their code and the data that they rely on that's what mlops is concerned itself with um what are the key steps involved in this first and foremost the model when it runs in production is presumably doing a scoring activity like or classifying or whatever right that model run is very important that you track that right you need to know when was a run done what data was a run on which parameters were set right beond the defaults and then the result that came from it the result could be as simple as you know just the score value or the uh could be an evaluation metric we spoke about evaluation metrices briefly at the end of the last video um or could be the you know data distributions the value the parameters Etc so tracking the model run is a key step that's involved in melops second thing is uh because it wants to con constrain the way in which models get deployed in production because you want to have control on that um the packaging of the code in order to create the models in the first place in various environments that is also part of mlops what does various environments mean and we'll take a look at some examples um in a couple of slides um it means that you're not going to be doing everything that you want to do for machine learning and production right um in production environment directly you will first start off in development environment which means you do some development on the code presumably in your jupyter notebook or in your local ID on your laptop um you test it out at small scale to just to you know make sure that it's doing the right thing and then once it's ready you put it into some kind of a cluster or an environment which trains on bigger data set and potentially you know a replica of the actual data set or as close to a replica as possible that environment is what we call a staging right and in staging you actually do the effective run Effectiveness runs the evaluation metries right all of those good things and it's in staging also that you spend time comparing it with other models potentially doing model tuning and parameter tuning hyper parameter tuning Etc all of that happens there and once that's done and staging and you've got the final model that's the final model that you then Shi to production right um so because you have to deal with multiple environments and the code itself moves to the staging and then finally the model gets moved to the production packaging the code becomes like an important consideration because at the end of the day almost any ml algorithm that you use today is not just a standalone code it's actually critically dependent on some libraries right either the algorithms themselves are part of some libraries like spark ml or pandas and umpai or you borrow it from open source like from third party contributors so um and the other consideration the other problems that we find here is that often times the software versions change what used to work in Python 2.x doesn't work in three anymore because the API standards have changed etc etc so what what is very important to package here is not just the code but also the artifacts the dependencies all of that which go around it um to make that code um working right so that's also part of the packaging the third uh step that's part of mlops is the management of the model artif s so here um what do the model artifacts comprise of um obviously it comprises of the actual model itself but the model might have seen many changes therefore you want to look at versioning every single change right attaching just a different version different nomenclature um even if it's the same n cature just a different version and use that um as a basis for tracking what's happening so versioning is an important consideration in terms of model artifacts documentation super important almost always we struggle with this or we tend to think of it post factor but mlops as a practice makes you think of it up front right make you think of it as part of the model deployment process and then finally when you think about deployment itself um you could actually have deployment into multiple different ways right one is a you know a scoring model deployed as a scoring API um so the model is deployed as a container or as an API and then whenever somebody needs a production from it or a score from it it goes um somebody makes u a request through and through rest and then gets the answer right in that case the serving interface for that model is actually an API in another case the model itself could be deployed as containers and the containers themselves are code that gets linked to the main code that are also running in containers um and then the serving interface becomes basically uh what's called as an RPC or a remote procedure call basically it's one process calling into another process as part of the same machine right that's so that case the RPC is the serving interface so bunch of different types in which possible pickle of course is a very common thing right uh so you package the pickle file as part of your uh scoring code and then um it gets invoked as part of that pipeline so deploy into these serving interfaces is a critical step see imagine if you had um a model that was deployed to an API the API is running actually at a remote endpoint um how do you control for that the API might have its own versions um and then the API request bodies could change because you've changed something there all of that is actually part and parel of the deployment of uh uh to the serving interface okay so now let's look at some of the problem statements that mlops tries to address um so one of the most common problem statements that you see once you start doing things in large scale is which code change that you made what parameters were used and what data inputs went into a particular result being computed so this is that that back example that we spoke about right why did that uh in customer get declined for a brand new credit card that was the result the decline was a result and we need to go back and look at what resulted in that particular result which means we need to go look at the data that was provided by the customer which is available right most um most banks have data governance systems so the data is available the parameters for the model are are also potentially available because it's running in production and the parameters don't change that often but what code change of the model produce the particular model file which run in production nobody knows right that was a problem um so here to to trace a particular result back to these these three parameters with these three factors is an important step um now things in in a jupyter notebook um the ease with which you work with notebooks is phenomenal right um so in fact I would say the Advent of a notebook interface became one of the key reasons why the the job title of a data scientist became so popular because you a data scientist who typically comes from a non-computer science background right uh could uh could end up actually doing um uh you know development of new models without having to understand the engineering chops right which means they don't have to understand how this code packaged how is you know bunch of different compilations happen uh what what role does environment play in in in actually ensuring that the compilation is successful and the run is successful how to troubleshoot and debug errors Etc bunch of different things that software Engineers take for granted or or or have learned as part of their career a data Cent typically does not get a chance to learn that and then for that data CST to contribute the notebook interface which gives instantaneous feedback which allows you to run uh without having to package things separately which are having to deploy things right separately all of that is very very powerful but the flip side to that is you don't actually have traceability right you don't really know what combination of steps resulted in a particular output that you see in the notebook so let's say you go through a multi-step notebook um and each step you run some um you know checks to ensure that the output that that's getting produced is what you expect so bunch of feature engineering steps you look at the features that get engineered everything looks legit to you you move on um then you start looking at the parameters that feed up into the model uh you check that and that looks fine then the model itself executes um and then you find that the you know FN score is not good right the evaluation metric FN score is not good so then you go back and change parameters um and then you run it again the FN score is still not good then you change a bunch of parameters like you do it like 10 15 times and then you find out that maybe the feature itself is wrong so you go back and do correlations as one of the steps that you missed out earlier then you remov some features and now you do this entire thing again bunch of 10 parameters that you changed and then finally The Notebook um uh notebook usage is over because you've arrived at a model which you think is the best but um if you then said okay look this seems like um the answer here is not as optimal as what I got like 10 runs ago what does 10 runs ago mean right in the context of a notebook what happened 10 turns ago was it just a case of running that particular cell in the notebook again and again or did you change some of the things and you you know you have to trace it back do that right so all of all of those became you know things that you had to you know really really care care about when you started to you know have multiple different versions of the um code that existed because the notebook uh you know way of coding allowed you to actually do changes on the Fly and not have to worry about it so that's one of the first problem statements that uh that you start tracking um that you start solving for with mlops the second uh motivation here is that the dependencies on libraries that you need for the models to execute those dependencies are notorious right and anybody who's done data science in handson will know it um you need to have all of the different uh versions of the code like even like I said the python 2.x was 3.0 that was a big change when they happened last year actually was it 2 years ago what yeah so it happened recently but that was a very big change and especially for ML um algorithms so when that happens um it becomes hard to reason about you know why did something fail in production uh because the environment is completely different right even though it's Linux even though it's python even though it's spark um despite all that there's a bunch of other dependencies like software versions software uh third party software Etc which are not being packaged correctly um this is one side of the story right the other part of it is you might actually end up coding uh some pretty wrong things into the code without knowing that these are not best practices so things like U you know having version controls for every single part of the change that you do which means that you make a change then you check it into a uh like a code versioning system like git right that allows you to keep track of every single change without having to worry about what change happened at that time which resulted in that particular result so those all become you know problem statements that you have to deal with uh then the way in which you package a model turns out to be quite um quite quite a challenge right in some cases uh you've seen people ship code right directly not the model because the code can be compiled in know different environment along with its dependencies so that the model gets generated a fresh that could work if the uh training is not a function right of uh the environment or the training can be made better because you have access to better data like is a case with like staging and uat environments which allow you to have a replica of the production data typically um but yes it's it's important to understand that um what you shi from one environment to another may not be actually producing the same model some therefore Some people prefer Uber jars Uber jars are basically one jar to rule them all right one jar to bring all the different other jars and and you know package it together and that's fine that works great but imagine if you have to start shipping around uber jars all the time these run into like hundreds of megabytes and like you know first you can't send it by email obviously but even sending it by SFTP results in you know somebody just waiting for some time before the transfer is over um and then somebody needs to keep track of it uh Version Control Systems with Uber jaars become very very uh sizable quickly right because every time you make a change you're actually versioning you know an Uber J which is like like 500 megabytes as an example so yeah so these things you have to pay attention to and that's something mlop SS you with and then finally evolving the models that's going into production um and understanding why did a Model evolve right what changed how did it uh change and tracking all those changes that's a challenge mlop helps you with that Cru so one of the most you know popular tools for mlops is a open source project called as ml flow and that's something that um ends up getting used all the time by many different software vendors so the I'll encourage you to take a look at it um we will spend some time on it in the next uh video thanks